---
Index: "[[🏷 Trouble Shooting]]"
tags:
  - Linux
  - Container
  - cgroup
  - Linux/ubuntu
  - cgroupv1
  - cgroupv2
status: Done
---
>[!Info] Link
>- [[02-06. cgroup의 이해]]
>- [[02-07-0. (실습) croup으로 Anti Virus 앱 CPU 사용 제한]]
>- [[02-07-b. apt error]]
>- [[02-07-c. 실습 환경 초기화]]
>- [[02-07-d. cgroupv2 -> v1 전환 방법 (in AWS EC2)]]

### 개요
- cgroup 관련 실습을 수행하려고 했다.
- 참고 자료를 따라, 진행하려고 했지만  `cgourp` 디렉터리 구조가 달랐다.
- 이를 추적해보니, 참고 자료는 ubuntu 20.04로 `cgourpv1` 이었고, 
  나는 ubuntu 24.04라 `cgourpv2` 였다.
- 각 `cgroup` 을 비교하고 정리한다.

---

ubuntu 24.04에서 `cgroup` 디렉토리 구조가 기존 20.04와 다르게 보이는 이유는 **기본적으로 `cgroup v2`가 적용되어 있기 때문**이다.
- ubuntu 20.04에서는 `cgroup v1`이 기본이다.
- ubuntu 22.04부터는 `cgroup v2`가 기본이다.

## 핵심 차이점 요약 (cgroup v1 : v2)

| 구분      | cgroup v1                         | cgroup v2                       |
| ------- | --------------------------------- | ------------------------------- |
| 디렉토리 구조 | `/sys/fs/cgroup/{cpu,memory,...}` | 단일 트리 구조: `/sys/fs/cgroup/` 하나뿐 |
| 컨트롤 방식  | 리소스별 하위 디렉토리 구성                   | 통합된 방식으로 리소스 관리                 |
| 명령어 호환성 | 일부 유틸리티는 v1 전용                    | systemd 및 새로운 도구 중심             |

---

## 현재 시스템의 cgroup 버전 확인 방법

```bash
mount | grep cgroup
```

- `cgroupv1`
	- `cgroup`이 여러 하위 리소스(cpu, memory 등)로 분리돼 있다.
```sh
mpfs on /sys/fs/cgroup type tmpfs (ro,nosuid,nodev,noexec,size=4096k,nr_inodes=1024,mode=755,inode64)
cgroup2 on /sys/fs/cgroup/unified type cgroup2 (rw,nosuid,nodev,noexec,relatime,nsdelegate)
cgroup on /sys/fs/cgroup/systemd type cgroup (rw,nosuid,nodev,noexec,relatime,xattr,name=systemd)
cgroup on /sys/fs/cgroup/hugetlb type cgroup (rw,nosuid,nodev,noexec,relatime,hugetlb)
cgroup on /sys/fs/cgroup/net_cls,net_prio type cgroup (rw,nosuid,nodev,noexec,relatime,net_cls,net_prio)
cgroup on /sys/fs/cgroup/blkio type cgroup (rw,nosuid,nodev,noexec,relatime,blkio)
cgroup on /sys/fs/cgroup/cpuset type cgroup (rw,nosuid,nodev,noexec,relatime,cpuset)
cgroup on /sys/fs/cgroup/misc type cgroup (rw,nosuid,nodev,noexec,relatime,misc)
cgroup on /sys/fs/cgroup/perf_event type cgroup (rw,nosuid,nodev,noexec,relatime,perf_event)
cgroup on /sys/fs/cgroup/cpu,cpuacct type cgroup (rw,nosuid,nodev,noexec,relatime,cpu,cpuacct)
cgroup on /sys/fs/cgroup/rdma type cgroup (rw,nosuid,nodev,noexec,relatime,rdma)
cgroup on /sys/fs/cgroup/freezer type cgroup (rw,nosuid,nodev,noexec,relatime,freezer)
cgroup on /sys/fs/cgroup/devices type cgroup (rw,nosuid,nodev,noexec,relatime,devices)
cgroup on /sys/fs/cgroup/pids type cgroup (rw,nosuid,nodev,noexec,relatime,pids)
cgroup on /sys/fs/cgroup/memory type cgroup (rw,nosuid,nodev,noexec,relatime,memory)
```


- cgroupv2
	- `type cgroup2` 가 표시된다.
```zsh
cgroup2 on /sys/fs/cgroup type cgroup2 (rw,nosuid,nodev,noexec,relatime,nsdelegate,memory_recursiveprot)
```

---


## `/sys/fs/cgroup/` 비교 (cgroupv1 : v2)

- `cgourpv1`
	- `sys/fs/croup/cpu` 디렉터리가 존재한다.
```bash
root@cgroupv1:/sys/fs/cgroup] ls
blkio        cpuacct  freezer  misc              net_prio    rdma
cpu          cpuset   hugetlb  net_cls           perf_event  systemd
cpu,cpuacct  devices  memory   net_cls,net_prio  pids        unified
```

- `cgroupv2`
	- `sys/fs/croup/cpu` 디렉터리가 없다.
		- `cgroupv2`에서는 `cgourpv1`과 달리,
		  리소스 종류별 디렉토리(`cpu`, `memory`, 등)가 존재하지 않는다.
	- `/sys/fs/cgroup/` 하나의 통합 디렉토리에서 `controllers` 파일을 통해 리소스를 제어하기 때문이다.
```sh
root@cgroupv2:/sys/fs/cgroup] ls
cgroup.controllers      cpuset.cpus.isolated   memory.reclaim
cgroup.max.depth        cpuset.mems.effective  memory.stat
cgroup.max.descendants  dev-hugepages.mount    memory.zswap.writeback
cgroup.pressure         dev-mqueue.mount       misc.capacity
cgroup.procs            init.scope             misc.current
cgroup.stat             io.cost.model          proc-sys-fs-binfmt_misc.mount
cgroup.subtree_control  io.cost.qos            sys-fs-fuse-connections.mount
cgroup.threads          io.pressure            sys-kernel-config.mount
cpu.pressure            io.prio.class          sys-kernel-debug.mount
cpu.stat                io.stat                sys-kernel-tracing.mount
cpu.stat.local          memory.numa_stat       system.slice
cpuset.cpus.effective   memory.pressure        user.slice
```


---

## cgroup 내 프로세스 등록 방식 비교 (cgroupv1 : v2)

| 버전        | 프로세스 등록 파일     | 설명              |
| --------- | -------------- | --------------- |
| cgroup v1 | `tasks`        | 프로세스/스레드 등록     |
| cgroup v2 | `cgroup.procs` | **프로세스만 등록** 가능 |
- cgroup 버전별 해당하는 파일에 PID를 추가하면, 해당 cgroup에 지정한 프로세스가 등록된다.


## 프로세스 등록 방법 비교
`tasks 또는 cgroup.procs`에 PID를 등록할 때, `>> 와 | tee` 모두 사용 가능하다.
하지만 권장 되는 방법은 `| tee` 이다.

| 방식    | 명령어                           | 설명                         | 주의사항                                        |
| ----- | ----------------------------- | -------------------------- | ------------------------------------------- |
| `tee` | `echo "+cpu" \| sudo tee ...` | stdout을 root 권한으로 직접 전달한다. | 안전한 방식이                                     |
| `>>`  | `echo "+cpu" >> ...`          | 파일에 문자열을 추가한다.             | 권한 문제 발생이 가능하다.<br>중복 `+cpu` 저장 시 오류가 발생한다. |

- `sudo echo ... >>`는 사실 **echo는 일반 사용자 권한**으로 실행되므로, 파일 쓰기 권한이 없으면 실패 한다. 또한,  `+cpu` 등 이미 리소스 컨트롤러를 다시 append할 경우 오류가 발생한다.
- 반면, `sudo tee`는 `tee` 자체가 루트 권한으로 파일을 연다. 더불어, 이미 동일한 컨트롤러가 추가돼 있으면 중복 에러 없이 무시된다.

## 올바른 방식 

```bash
echo "+cpu" | sudo tee /sys/fs/cgroup/cgroup.subtree_control
```

- 필요 시, 여러 개도 가능하다.
```bash
echo "+cpu +memory" | sudo tee /sys/fs/cgroup/cgroup.subtree_control
```


---

##  기타 참고 파일 (cgroup v2)

| 파일명                  | 설명                                       |
| -------------------- | ---------------------------------------- |
| `cgroup.procs`       | `cgroup`에 포함된 프로세스의 PID 목록이 있는 파일        |
| `cgroup.controllers` | `cgroup`에서 활성화할 수 있는 리소스 컨트롤러 목록이 있는 파일  |
| `cgroup.subtree`     | 자식 `cgroup`에 어떤 리소스 컨트롤러를 활성화할 지 결정하는 파일 |
| `cpu.max`            | CPU 제한 (quota/period)                    |
| `cpu.stat`           | 사용된 시간, throttling 정보 등 제공               |
| `cpu.weight`         | 스케줄링 가중치 (v1의 shares 대체)                 |


---

## 실습 (cgroup v2 기준)

^c4b34d


### 1. 실습을 위한 `stress`  설치

```bash
sudo -i
apt-get update
apt-get install -y stress
stress --version
```

### 2. cgroup 하위 그룹 생성

```bash
cd /sys/fs/cgroup
mkdir test
```

### 3. cgroup에 task 등록

```sh
echo $$ | sudo tee /sys/fs/cgroup/test/cgroup.procs
1248

cat /test/cgroup.procs
1248
1909
```
- `$$`는 현재 셸의 PID를 의미한다. 
- `/sys/fs/cgroup/cgroup.procs`
	- 해당 `cgroup`에 포함된 프로세스의 PID 목록이 있는 파일이다.
	- 해당 파일에 특정 프로세스의 PID를 작성하면, 해당 `cgroup`으로 이동한다.
- 즉, 현재 셸 프로세스를 새로 생성한 `test cgroup`으로 이동시키는 것이다.

### 4. 컨트롤러 확인, 활성 컨트롤러 설정
- `/sys/fs/cgroup/cgroup.controllers`
	- 현재 cgroup에서 활성화할 수 있는 리소스 컨트롤러 목록이 있는 '읽기 전용 파일'이다. 
- `/sys/fs/cgroup/cgroup.subtree_control`
	- 자식 `cgroup`에 어떤 리소스 컨트롤러를 활성화할 지 결정하는 파일이다.
	- `+ 또는 -` 기호와 함께 컨트롤러를 작성하여 자식 `cgroup`에서 사용할 수 있는 컨트롤러를 활성화/비활성화 할 수 있다.
	- 부모 `cgroup`에서 활성화한 컨트롤러만 자식 cgroup에서도 사용 가능하다.

```bash
# 활성화 가능한 리소스 컨트롤러 목록 확인
cat /sys/fs/cgroup/cgroup.controllers
cpu memory pids

# 현재 활성화된 컨트롤러 목록 확인
cat /sys/fs/cgroup/cgroup.subtree_control

# cpu 리소스 컨트롤러 활성화
echo "+cpu" | sudo tee /sys/fs/cgroup/cgroup.subtree_control
```


### 5. CPU 리소스 제한 설정
- `cpu.max`
	- 해당 `cgroup`에 할당할 수 있는 CPU 제한을 설정하는 파일이다.
		- 형식 : `<quota> <period>`
			- 시스템 기본값 : `max 100000`
```bash
# 시스템 기본값 확인
cat /sys/fs/cgroup/test/cpu.max
max 100000

# test cgroup에 CPU 제한 설정
# 100,000 마이크로초(0.1초)마다 최대 100,00(0.01초) 동안 CPU 사용 가능
# => 10% CPU 사용 가능
echo 10000 100000 | sudo tee /sys/fs/cgroup/test/cpu.max  
```


## 6. 결과
- cgroup 적용 전 `stress`
	- `%CPU`가 제한 없이 `99.7`까지 사용되는 것을 확인 할 수 있다.
![[Pasted image 20250510003407.png]]

- cgroup 적용 후 `stress`
	- `stress` 의 `%CPU` 가 10으로 제한이 걸리는 것을 확인할 수 있다.

![[스크린샷 2025-05-10 00.35.21.png]]